# BackdoorAlign trainer configuration (single GPU example)
defender_type: BackdoorEnhancedAlignmentTrainer

# Training entry point and interpreter
train_script: finetuning.py
python_executable: python3

# The bundled assets are used when asset_root is omitted.
asset_root: "/gemini/code/tele/telesafety_defense/method_utils/Backdoor-Enhanced-Alignment/opensource"

# Core training hyper-parameters (mirrors run_backdooralign.sh)
model_name: "TheBloke/Llama-2-7b-chat-fp16"
batch_size_training: 10
lr: 5e-5
num_epochs: 5
gradient_accumulation_steps: 1
run_validation: false
enable_fsdp: true
pure_bf16: true
seed: 42

# Dataset options
dataset: pure_bad_dataset
mix_type: random150
train_split: "ft_datasets/pure_bad_dataset/pure_bad_100_original_category_prefixed.jsonl"

# Output locations
dist_checkpoint_root_folder: "finetuned_models"
dist_checkpoint_folder: "backdooralign"
output_dir: "finetuned_models/backdooralign-temp_model"
final_model_dir: "finetuned_models/backdooralign"
convert_checkpoint: true

# Conversion arguments (converts FSDP shards to standard HF format)
convert_script: "inference/checkpoint_converter_fsdp_hf.py"
convert_kwargs:
  fsdp_checkpoint_path: "finetuned_models/backdooralign-temp_model"
  consolidated_model_path: "finetuned_models/backdooralign"
  HF_model_path_or_name: "TheBloke/Llama-2-7b-chat-fp16"

# Optionally remove temporary shards after conversion
cleanup_paths:
  - "finetuned_models/backdooralign-temp_model"

"""
Models Module for Telesafety Defense
====================================

This module provides model loading and management functionality.
"""
from fastchat.conversation import get_conv_template
import torch
from tqdm import trange
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Dict, Any, Optional


class LocalModel:
    """Wrapper for local models."""
    
    def __init__(self, model, tokenizer, model_name, generation_config):
        self.model = model
        self.tokenizer = tokenizer
        self.model_name = model_name
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.pos_to_token_dict = {v: k.replace('▁', ' ') for k, v in self.tokenizer.get_vocab().items()}
        self.generation_config = generation_config or {}
        _model_name = model_name
        if 'vicuna' in model_name:
            _model_name = 'vicuna_v1.1'
        self.conversation = get_conv_template(_model_name)
        self.device = next(model.parameters()).device

    def chat(self, messages, **kwargs):
        """Chat interface for the model."""
        # Combine generation config with kwargs
        
        # Format messages for the model
        if isinstance(messages, str) and use_chat_template == True:
            messages = [
                {
                    "role": "user",
                    "content": messages
                }
            ]
        
        prompt = self.apply_chat_template(messages)

        # ===================== 在这里加上调试代码 =====================
        # print("="*80)
        # print("DEBUG: Final prompt being sent to the model:")
        # print(repr(prompt))  # 使用 repr() 可以看到换行符等特殊字符
        # print("="*80)
        # ==========================================================
        
        # Tokenize input
        inputs = self.tokenizer([prompt], return_tensors="pt",add_special_tokens=False).to(self.device)
        torch.cuda.empty_cache()
        
        temp_generation_config = self.generation_config.copy()
        
        if "generation_config" in kwargs:
            temp_generation_config = kwargs["generation_config"]
        else:
            temp_generation_config = self.generation_config.copy()
            temp_generation_config.update(kwargs)
        # Generate response
        with torch.no_grad():
            # out = self.model.generate(**inputs, **temp_generation_config, max_new_tokens=100)
            # 如果 temp_generation_config 里没有 max_new_tokens，才设置
            if "max_new_tokens" not in temp_generation_config:
                temp_generation_config["max_new_tokens"] =512
            out = self.model.generate(**inputs, **temp_generation_config)
        
        # Decode response
        response = self.tokenizer.decode(
            out[0][len(inputs["input_ids"][0]):], 
            skip_special_tokens=True
        )

        # ===================== 在这里加上新的调试代码 =====================
        # print("+"*80)
        # print("DEBUG: Raw decoded text output from the model:")
        # print(repr(response)) # 同样使用 repr() 方便查看所有字符
        # print("+"*80)
        # =============================================================
        
        return response.strip()
    
    def generate(self, input_ids, **kwargs):
        """Generate text from input IDs."""
        gen_config = {**self.generation_config, **kwargs}
        if isinstance(input_ids, dict):
            input_ids = input_ids["input_ids"]
        elif isinstance(input_ids, list):
            input_ids = torch.tensor(input_ids, dtype=torch.long)
        input_ids = input_ids.to(self.model.device)
        if gen_config is None:
            gen_config = getattr(self.model, "generation_config", None)
        if gen_config is not None:
            if not hasattr(gen_config, "max_new_tokens") or gen_config.max_new_tokens is None:
                gen_config.max_new_tokens = 128

        attn_mask = torch.ones_like(input_ids).to(self.model.device)
        model_type = getattr(self, "model_name", None)
        with torch.no_grad():
            output_ids = self.model.generate(
                input_ids,
                attention_mask=attn_mask,
                generation_config=gen_config,
                pad_token_id=self.tokenizer.pad_token_id
            )[0]
        
        return output_ids[input_ids.size(1):]
    
    def batch_chat(self, batch_messages, batch_size=8, skip_special_tokens=True, **kwargs):
        prompts = []
        for messages in batch_messages:
            prompt = self.apply_chat_template(messages)
            prompts.append(prompt)

        responses = []
        temp_generation_config = self.generation_config.copy()
        
        if "generation_config" in kwargs:
            temp_generation_config = kwargs["generation_config"]
        else:
            temp_generation_config = self.generation_config.copy()
            for k in kwargs:
                if k in self.generation_config.keys():
                    setattr(temp_generation_config, k, kwargs[k])
        
        for i in trange(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i + batch_size]
            inputs = self.tokenizer(batch_prompts, return_tensors='pt', padding=True, add_special_tokens=False).to(self.device)
            out = self.model.generate(**inputs, **temp_generation_config)
            for j, input_ids in enumerate(inputs["input_ids"]):
                response = self.tokenizer.decode(out[j][len(input_ids):], skip_special_tokens=skip_special_tokens)
                responses.append(response)

        return responses

    def apply_chat_template(self, messages):
        import copy
        if isinstance(messages, str):
            msgs = [{"role": "user", "content": messages}]
        
        else:
            msgs = copy.deepcopy(list(messages))
        
        prefill = False
        if msgs[-1]['role'] == 'assistant':
            prefill = True
        
        try:
            # first try the model's own tokenizer
            if prefill:
                prompt = self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)
            else:
                prompt = self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        
        except Exception as e:
            conversation = self.conversation.copy()

            tmp = msgs
            if tmp[-1]["role"] != 'assistant':
                tmp = tmp + [{"role": "assistant", "content": None}]
        
            if tmp[0]["role"] == 'system':
                conversation.set_system_message(tmp[0]['content'])
                tmp = tmp[1:]
            for msg in tmp:
                conversation.append_message(msg['role'], msg['content'])
            
            prompt = conversation.get_prompt()
            if conversation.name == 'vicuna_v1.1':
                prompt = prompt.replace('user:', 'User:').replace('assistant:', 'ASSISTANT:')
            
            
        if self.tokenizer.bos_token and prompt.startswith(self.tokenizer.bos_token):
            # if there are two bos tokens, remove one
            # prompt = prompt.replace(self.tokenizer.bos_token, '', 1).lstrip()
            prompt = prompt.replace(self.tokenizer.bos_token, '', 1)
        
        if self.tokenizer.bos_token and not prompt.startswith(self.tokenizer.bos_token):
            prompt = self.tokenizer.bos_token + prompt

        if prefill:
            if self.tokenizer.eos_token and prompt.strip().endswith(self.tokenizer.eos_token):
                idx = prompt.rindex(self.tokenizer.eos_token)
                prompt = prompt[:idx].rstrip()
            
        return prompt
def load_model(model=None, tokenizer=None, model_name: str = "unknown", 
               model_path: str = None, api_key: str = None, base_url: str = None,
               generation_config: Optional[Dict] = None, **kwargs) -> LocalModel:
    """
    Load a model for defense evaluation.
    
    Args:
        model: Pre-loaded model (optional)
        tokenizer: Pre-loaded tokenizer (optional)
        model_name: Name of the model
        model_path: Path to the model (for local models)
        api_key: API key (for remote models)
        base_url: Base URL (for remote models)
        generation_config: Generation configuration
        **kwargs: Additional arguments
        
    Returns:
        LocalModel: Wrapped model instance
    """
    if model is not None and tokenizer is not None:
        # Use pre-loaded model and tokenizer
        return LocalModel(model, tokenizer, model_name, generation_config)
    
    elif model_path:
        # Load local model
        if not model:
            model = AutoModelForCausalLM.from_pretrained(
                model_path, 
                torch_dtype=torch.float16,
                device_map='auto',
                trust_remote_code=True
            ).eval()
        
        if not tokenizer:
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
            )
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                tokenizer.pad_token_id = tokenizer.eos_token_id
        
        return LocalModel(model, tokenizer, model_name, generation_config)
    
    elif api_key and base_url:
        # For remote models, create a placeholder
        # In a real implementation, you would connect to the remote service
        raise NotImplementedError("Remote model loading not yet implemented")
    
    else:
        raise ValueError("Must provide either model+tokenizer or model_path or api_key+base_url")

"""
Semantic SmooothLLM Defense Method
============================================
This Class achieves a defense method describe in the paper below.
This part of code is based on the code from the paper.

Paper title: Certifying LLM Safety against Adversarial Prompting
Arxiv link: https://arxiv.org/abs/2402.16192
Source repository: https://github.com/UCSB-NLP-Chang/SemanticSmooth
"""
from telesafety_defense.base_factory import OutputDefender
from aisafetylab.models import LocalModel
from telesafety_defense.utils import DictScorer
from telesafety_defense.utils.prompts import SORRY_RESPONSE, PARAPHRASE_TEXT, SPELLCHECK_TEXT, SUMMARIZE_TEXT, SYNONYM_TEXT, TRANSLATION_TEXT, VERTTENSE_TEXT
from loguru import logger
import random
import os
import numpy as np
import json
import tiktoken

class SemanticSmoothLLMDefender(OutputDefender):
    def __init__(
        self,
        model,
        model_name,
        tokenizer,
        scorer=DictScorer(), 
        pert_type='random', 
        num_samples=3,
        batch_size=1, 
        generation_config=None):
        """
        Args:
            scorer (PatternScorer): Scorer to evaluate if a response is harmful.
            per_type (str): Type of perturbation to apply.
            num_samples (int): Number of perturbed samples to generate.
            batch_size (int): Batch size for processing.    
        """
        # Keep raw HF model; runtime wrapper is provided by caller in `defend`
        self.model = model
        self.model_name = model_name
        self.tokenizer = tokenizer
        self.scorer = scorer
        self.pert_type = pert_type
        self.num_samples = num_samples
        self.batch_size = batch_size
        # Default to an empty config so downstream callers never receive None
        self.generation_config = generation_config or {}
        logger.info(
            f"SemanticSmoothLLMDefender initialized with pert_type: {self.pert_type}, "
            f"num_samples: {self.num_samples}, batch_size: {self.batch_size}"
        )

    def random_perturb(self, harmful_prompt):
        """
        Args:
            harmful_prompt (str): The harmful prompt to be perturbed.
        
        Returns:
            str: The perturbed prompt.
        """
        perturbation_list = ["paraphrase", "spellcheck", "summarize", "synonym", "translation", "verbtense"]
        
        if self.pert_type == 'random':
            self.pert_type = random.choice(perturbation_list)

        if self.pert_type in perturbation_list:
            return self.perturb(self.pert_type, harmful_prompt)
        else:
            raise NotImplementedError(f"{self.pert_type} is not implemented!")
        
    def perturb_with_llm(self, template, harmful_prompt):
        """
        Args:
            template (str): The template to use for perturbation.
            harmful_prompt (str): The harmful prompt to be perturbed.
        
        Returns:
            str: The perturbed prompt generated by the LLM.
        """
        prompt = template.replace('{QUERY}', harmful_prompt)
        # Use the runtime model wrapper passed in `defend`
        runtime_model = getattr(self, "_runtime_model", None)
        if runtime_model is None:
            # Fallback: create a temporary LocalModel wrapper if needed
            runtime_model = LocalModel(self.model, self.tokenizer, self.model_name, self.generation_config)
        return runtime_model.chat([
            {"role": "user", "content": prompt}
        ], generation_config=self.generation_config)
    
    def perturb(self, pert_type, harmful_prompt):
        """
        Args:
            pert_type (str): Type of perturbation to apply.
            harmful_prompt (str): The harmful prompt to be perturbed.
        
        Returns:
            str: The perturbed prompt.
        """
        pert_type = pert_type.upper()
        perturbation_template = f"{pert_type}_TEXT"
        return self.perturb_with_llm(perturbation_template, harmful_prompt)
    
    def extract_response(self, outputs):
        """
        Args:
            outputs (list): List of outputs generated by the model.
        
        Returns:
            list: List of extracted responses.
        """
        res = []
        for x in outputs:
            try:
                start_pos = x.find("{")
                if start_pos == -1:
                    x = "{" + x
                x = x[start_pos:]
                end_pos = x.find("}") + 1
                if end_pos == -1:
                    x = x + " }"
                jsonx = json.loads(x[:end_pos])
                for key in jsonx.keys():
                    if key != "format":
                        break
                outobj = jsonx[key]
                if isinstance(outobj, list):
                    res.append(" ".join([str(item) for item in outobj]))
                else:
                    if "\"query\":" in outobj:
                        outobj = outobj.replace("\"query\": ", "")
                        outobj = outobj.strip("\" ")
                    res.append(str(outobj))
            except Exception as e:
                x = x.replace("{\"replace\": ", "")
                x = x.replace("{\"rewrite\": ", "")
                x = x.replace("{\"fix\": ", "")
                x = x.replace("{\"summarize\": ", "")
                x = x.replace("{\"paraphrase\": ", "")
                x = x.replace("{\"translation\": ", "")
                x = x.replace("{\"reformat\": ", "")
                x = x.rstrip("}")
                x = x.lstrip("{")
                x = x.strip("\" ")
                res.append(x.strip())
                continue
        return res
    
    def defend(self, model, messages):
        # record runtime LocalModel wrapper for perturbations
        self._runtime_model = model
        if isinstance(messages, str):
            messages = [
                {
                    "role": "user",
                    "content": messages
                }
            ]
        
        prompt = messages[-1]['content']
        all_inputs = [
            self.random_perturb(prompt)
            for _ in range(self.num_samples)
        ]
        all_inputs = self.extract_response(all_inputs)

        all_outputs = []
        for i in range(self.num_samples // self.batch_size + 1):
            batch = all_inputs[i * self.batch_size:(i + 1) * self.batch_size]
            batch_messages = [messages.copy() for _ in batch]
            for j, perturbed_prompt in enumerate(batch):
                batch_messages[j][-1]['content'] = perturbed_prompt

            batch_outputs = model.batch_chat(batch_messages)
            tmp = []

            for output in batch_outputs:
                try:
                    response = output
                except:
                    response = SORRY_RESPONSE
                    print(f"Batch Generation failed while return {output}.")

                tmp.append(response)
            all_outputs.extend(tmp)
        
        are_copies_jailbroken = [self.scorer.score("", output)['score'] for output in all_outputs]

        outputs_and_jbs = list(zip(all_outputs, are_copies_jailbroken))

        jb_percentage = np.mean(are_copies_jailbroken)
        smooth_llm_jb = 1 if jb_percentage > 0.5 else 0

        majority_outputs = [
            output for (output, jb) in outputs_and_jbs
            if jb == smooth_llm_jb
        ]

        chosen_output = random.choice(majority_outputs) if majority_outputs else prompt

        return chosen_output

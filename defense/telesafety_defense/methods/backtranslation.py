"""
BackTranslation Defense Method
============================================
This Class achieves a defense method describe in the paper below.

Paper title: Defending LLMs against Jailbreaking Attacks via Backtranslation
Arxiv link: https://arxiv.org/abs/2402.16459
Source repository: https://github.com/YihanWang617/llm-jailbreaking-defense
"""

from aisafetylab.defense.inference_defense.base_defender import IntraprocessDefender
from aisafetylab.models import LocalModel, AzureOpenAIModel
from aisafetylab.evaluation.scorers import PatternScorer
from aisafetylab.defense.inference_defense.defender_texts import SORRY_RESPONSE, BACK_TRANSLATION_DEFAULT_TEXT
from loguru import logger
import math
import tiktoken
import re

class BackTranslationDefender(IntraprocessDefender):
    def __init__(self, scorer=PatternScorer(), threshold=-2.0, infer_prompt=BACK_TRANSLATION_DEFAULT_TEXT, model_name="vicuna-13b", model=None, tokenizer=None):
        """
        Args:
            scorer (PatternScorer): Scorer to evaluate if a response is harmful.
            threshold (float): Threshold for determining the likelihood of harmful prompts.
            infer_prompt (str): The prompt used to infer the user's original request.
            model_name (str): Name of the LLM used for inference.
            model (Model): The inference model(used for backtranslation).
            tokenizer: The tokenizer corresponding to the model.
            base_url (str): Base URL for the OpenAI API.
            api_key (str): API key for OpenAI.
        """
        self.scorer = scorer
        self.threshold = threshold
        self.infer_prompt = infer_prompt
        self.model = LocalModel(model, tokenizer, model_name)
        logger.info(f"BackTranslationDefender initialized with model: {model_name}, threshold: {threshold}")
    
    def infer_question(self, response):
        """
        Infer the user's original request based on the AI assistant's response.

        Args:
            Response generated by the assistant.

        Returns:
            str: The inferred user request.
        """
        infer_messages = [
            {
                "role": "user",
                "content": self.infer_prompt.replace("response", f"{response}")
            }
        ]
        infer_request = self.model.chat(infer_messages)

        infer_request = infer_request.split(':')[-1].split("\n")[0].strip().strip(']').strip('[')
        logger.info(f"Inferred request: {infer_request}")
        return infer_request

    def filter_question_with_likelihood(self, prompt, response):
        """
        Filter the inferred prompt based on its likelihood to prevent harmful content.

        Args:
            prompt (str): The inferred user request.
            response (str): The response generated by the assistant.

        Returns:
            bool: True if the prompt is considered safe, False otherwise.
        """
        if self.threshold > -math.inf:
            avg_log_likelihood = self.model.evaluate_log_likelihood(
                [
                    {"role": "user", "content": prompt},
                    {"role": "assistant", "content": response}
                ]
            )
            return sum(avg_log_likelihood) / len(avg_log_likelihood) > self.threshold
        else:
            return True

    def defend(self, model, messages):
        # Ensure messages is in the correct format
        if isinstance(messages, str):
            messages = [
                {
                    "role": "user",
                    "content": messages
                }
            ]
        
        # Generate the orignial response
        response = model.chat(messages).strip()
        response_score = self.scorer.score(query=messages[-1]['content'], response=response)['score']

        logger.info(f"Original Query: {messages[-1]['content']}")
        logger.info(f"Original response: {response}")

        if response_score == 0 or response == "":
            # Original response is harmful
            return SORRY_RESPONSE
        
        possible_prompt = self.infer_question(response)
        is_reasonable = self.filter_question_with_likelihood(possible_prompt, response)

        if not is_reasonable:
             return response
        
        new_response = model.chat(
            [
                {
                    "role": "user",
                    "content": possible_prompt
                }
            ]
        )

        new_response_score = self.scorer.score(query=possible_prompt, response=new_response)['score']
        
        if new_response_score == 0:
            return SORRY_RESPONSE
        else:
            return response
